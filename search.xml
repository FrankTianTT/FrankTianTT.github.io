<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Distributional DQN</title>
    <url>/2020/12/10/Distributional%20DQN/</url>
    <content><![CDATA[<p>这篇文章介绍了关于Distributional DQN的三篇文章，这三篇论文的作者差不多是同一拨人，都来自Deep Mind，之后微软还有一篇后续的工作，有时间再贴上来。</p>
<p>所谓的Distributional DQN，指的是对于一个<span class="math inline">\((s,a)\)</span>对，我们不再用一个<strong>值</strong>来评估它，而是使用一个<strong>分布</strong>去评价这个状态和动作的好坏。我们用<span class="math inline">\(Z(s,a)\)</span>表示这个分布，而之前的<span class="math inline">\(Q(s,a)\)</span>则是<span class="math inline">\(Z(s,a)\)</span>的<strong>期望</strong>。</p>
<a id="more"></a>
<h2 id="用bellman算子理解动态规划">用Bellman算子理解动态规划</h2>
<p>如果你对强化学习本身已经非常熟悉，可以考虑跳过这一节的内容。</p>
]]></content>
  </entry>
  <entry>
    <title>Hexo中marked和mathjax冲突的问题</title>
    <url>/2020/12/10/Hexo%E4%B8%ADmarked%E5%92%8Cmathjax%E5%86%B2%E7%AA%81%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>最近打算在博客上整理一些论文笔记，发现hexo默认支持的marked渲染和我附加的mathjax渲染是冲突的，主要有包括两个符号，<code>\</code>和<code>_</code>。</p>
<a id="more"></a>
<p>网上冲浪了一下找到了三种解决方案，分别是</p>
<ul>
<li>手动在<code>\</code>和<code>_</code>前面添加转义符，或者在公式前后用保护块，其实用python正则表达式替换一下也不麻烦，但是我是强迫症，放弃。</li>
<li>更改marked.js的代码，把marked处理<code>\</code>和<code>_</code>的部分去掉，留着这两个符号给mathjax处理，但是我是强迫症，放弃。</li>
<li>使用pandoc，把原来的marked卸载掉。</li>
</ul>
<p>显然最后我选择了第三种方法。</p>
<p>主要就是卸载掉原来的渲染引擎<code>npm uninstall hexo-renderer-marked</code>，并安装pandoc<code>npm install hexo-renderer-pandoc</code>。注意pandoc是需要安装在本地的，在ubuntu下用<code>apt</code>安装就好了。在Mac上还有一个坑，我在下载Anaconda的时候它自带了一个版本很久的pandoc，和Hexo使用的pandoc有冲突，需要卸载掉Anaconda自带的，然后手动去官网上安装。</p>
<p>解决问题后我又找到了NexT官方给出的解决方案<a href="https://github.com/theme-next/hexo-theme-next/blob/master/docs/zh-CN/MATH.md">数学公式</a>，不过里面有一些东西已经随着NexT本身的更新消失了。</p>
<p>总之就是非常烦。</p>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>不做程序员</tag>
        <tag>写个博客怎么这么多坑</tag>
      </tags>
  </entry>
  <entry>
    <title>建站攻略</title>
    <url>/2020/12/08/%E5%BB%BA%E7%AB%99%E6%94%BB%E7%95%A5/</url>
    <content><![CDATA[<p>让我来总结一下这个网站的主要技术。</p>
<p>总的来说，就是用Hexo在阿里云的服务器渲染网站页面，同时又把页面的提交到了github page和gitee page上，让这两个网站也可以同步我的网站。最后实现了自动pull和push的功能，让我可以在我的Mac上专注写文字，然后用一个脚本解决所有问题。</p>
<a id="more"></a>
<p>在这里我们不贴一堆乱七八糟的代码了，把主要流程和借鉴的博客贴上来好了。</p>
<p>首先是在Linux上建立Hexo，要安装nvm和Node.js，以及包管理工具npm。服务器上可能无法访问raw.githubusercontent.com，可以参考GitHub520这个项目。然后就是建立目录，初始化Hexo项目，再把NexT这个主题放到对应的目录下。随便改一改主题，挑一个自己喜欢的。然后可以把github和gitee的master分支加入Hexo的deploy配置中，而整个项目保存在dev分支中。</p>
<p>这部分主要参考<a href="https://blog.csdn.net/u010725842/article/details/80672739">Linux下使用Hexo搭建github博客</a>和<a href="https://blog.csdn.net/weixin_44555878/article/details/106588253">hexo博客部署并同步更新到服务器</a>。Hexo和NexT的用法很容易搜到。</p>
<p>此外，在阿里云中将端口号映射为二级域名可以通过<a href="https://blog.csdn.net/zz_aiytag/article/details/108868654">阿里云二级域名解析到指定端口号的一种方法</a>实现。</p>
<p>接下来在本地clone下来，写个脚本自动登陆到远程的服务器上，从gitee的dev上pull下来，然后push到github上的dev中。最后<code>hexo generate</code>和<code>hexo deploy</code>。</p>
<p>我是用的是<code>expect</code>脚本登陆服务器，<code>git pull</code>要输入密码的话也可以用<code>expect</code>脚本。<code>expect</code>脚本的教程也很好找，一个值得注意的坑是<code>expect</code>的转义字符，例如<code>$</code>是<code>\\\$</code>，详见<a href="https://blog.csdn.net/secondjanuary/article/details/21775953">expect需要转义的符号列表</a>。</p>
<p>在服务器上<code>git pull</code>可能会有冲突，可以先<code>git fetch</code>然后<code>git --hard reset gitee/dev</code>最后再<code>git pull</code>。他们的区别可以看<a href="https://www.cnblogs.com/runnerjack/p/9342362.html">git fetch &amp; pull详解</a>。</p>
<p>此外还有一些坑，例如hexo有一些依赖库要装，百度上都能搜到我就不记录了，毕竟我又不是程序员。</p>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>不做程序员</tag>
      </tags>
  </entry>
  <entry>
    <title>用Bellman算子理解动态规划</title>
    <url>/2020/12/10/%E7%94%A8Bellman%E7%AE%97%E5%AD%90%E7%90%86%E8%A7%A3%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</url>
    <content><![CDATA[<p>这片文章算是对Distributional DQN的补充内容，介绍了一些value-based的基本概念以及收敛性的证明。</p>
<p>下面的内容主要是对这个<a href="https://web.stanford.edu/class/cme241/lecture_slides/BellmanOperators.pdf">slides</a>的整理。</p>
<a id="more"></a>
<h2 id="value-functions-as-vectors">Value Functions as Vectors</h2>
<p>首先，我们尝试把一个<strong>函数</strong>看作一个<strong>向量</strong>，这可能是泛函中最基本的概念。</p>
<p>我们假设在状态空间<span class="math inline">\(\mathcal{S}\)</span>中有<span class="math inline">\(n\)</span>个状态</p>
<p><span class="math inline">\(\left\{s_{1}, s_{2}, \dots, s_{m}\right\}\)</span></p>
<p>在动作空间<span class="math inline">\(\mathcal{A}\)</span>中有<span class="math inline">\(m\)</span>个状态</p>
<p><span class="math display">\[\left\{a_{1}, a_{2}, \dots, a_{m}\right\}\]</span></p>
<p>当然在连续的情形就是<span class="math inline">\(n = \infty\)</span>或者<span class="math inline">\(m = \infty\)</span>，不过它们依旧可以表示为一系列向量。</p>
<p>我们的随机策略定义为<span class="math inline">\(\pi(a | s)\)</span>。</p>
<p>那么值函数定义为<span class="math inline">\(\mathbf{v}: \mathcal{S} \to \mathbb{R}\)</span>，当然值函数是针对某个策略<span class="math inline">\(\pi\)</span>而言的，也就是</p>
<p><span class="math display">\[\mathbf{v}_{\pi}: \mathcal{S} \to \mathbb{R}\]</span></p>
<p>最优值函数被定义为</p>
<p><span class="math display">\[\mathbf{v}_{*}(s)=\max _{\pi} \mathbf{v}_{\pi}(s)\]</span></p>
<p>定义$_{s}^{a} <span class="math inline">\(是在状态\)</span>s<span class="math inline">\(做动作\)</span>a$得到奖赏的期望。</p>
<p>定义<span class="math inline">\(\mathcal{P}_{s, s^{\prime}}^{a}\)</span>是在状态<span class="math inline">\(s\)</span>做动作<span class="math inline">\(a\)</span>到达状态<span class="math inline">\(s^\prime\)</span>的概率。</p>
<p>那么我们可以用<span class="math inline">\(\mathbf{R}_{\pi}(s)\)</span>表示对于这个策略<span class="math inline">\(\pi\)</span>，在状态<span class="math inline">\(s\)</span>下可能获得的“奖赏的期望”的期望。（第一个期望来自环境的不确定性，第二个期望来自策略的不确定性）</p>
<p><em>{}(s)=</em>{a<span class="math inline">\(\in \mathcal{A}} \pi(a | s) \cdot \mathcal{R}_{s}^{a}\)</span></p>
<p>用<span class="math inline">\(\mathbf{P}_{\pi}\left(s, s^{\prime}\right)\)</span>表示对于这个策略<span class="math inline">\(\pi\)</span>，从状态<span class="math inline">\(s\)</span>转移到状态<span class="math inline">\(s^\prime\)</span>的概率。</p>
<p>_{}(s,<span class="math inline">\(s^{\prime}\right)=\sum_{a \in \mathcal{A}} \pi(a | s) \cdot \mathcal{P}_{s, s^{\prime}}^{a}\)</span></p>
<p>我们用<span class="math inline">\(\mathbf{R}_{\pi}\)</span>表示向量<span class="math inline">\(\left[\mathbf{R}_{\pi}\left(s_{1}\right), \mathbf{R}_{\pi}\left(s_{2}\right), \ldots, \mathbf{R}_{\pi}\left(s_{n}\right)\right]\)</span>，用<span class="math inline">\(\mathbf{P}_{\pi}\)</span>表示矩阵, 1 i, i^{} n$。</p>
<p>定义<span class="math inline">\(\gamma\)</span>为MDP折扣因子。</p>
<p>Bellman Operators</p>
<p>补充，算子operator，可以粗浅的理解为</p>
<p>函数的输入是集合，输出是集合</p>
<p>泛函的输入是函数，输出是集合</p>
<p>算子的输入是函数，输出是函数</p>
<p>Bellman$Policy Operator _{} <span class="math inline">\(作为一个算子，是针对策略\)</span><span class="math inline">\(而言的，它作用于函数\)</span>v$之上</p>
<p><em>{}$=</em>{}+_{} </p>
<p><em>{} <span class="math inline">\(是一个线性的算子，有不动点\)</span>v</em><span class="math inline">\(，即\)</span><em>{} </em>{}=_{}$。</p>
<p>这个不动点由$<em>{} <span class="math inline">\(决定，本质上是由\)</span></em>{} <span class="math inline">\(，\)</span>_{} <span class="math inline">\(和\)</span>$决定。</p>
<p>还有算子Bellman<span class="math inline">\(Optimality Operator \mathbf{B}_{*}\)</span>，被定义为</p>
<p>(_{*}$)(s)=<em>{a}{</em>{s}^{a}+<em>{s^{} } </em>{s, s<sup>{}}</sup>{a} (s^{})}</p>
<p>_{*}$并不依赖于某个具体的策略。</p>
<p>_{*} 也有不动点<span class="math inline">\(\mathbf{v}_{*}\)</span>，满足_{*}$_{*}=_{*} $。</p>
<p>这个不动点由<span class="math inline">\(\mathbf{B}_{*}\)</span>决定，本质上是由$<em>{s}^{a} <span class="math inline">\(，\)</span></em>{s, s<sup>{}}</sup>{a}<span class="math inline">\(和\)</span>$决定。</p>
<p>接下来我们定义一个greedy的策略<span class="math inline">\(G(\mathbf{v})(s)\)</span>，<span class="math inline">\(G\)</span>是一个算子，输入是函数<span class="math inline">\(\mathbf{v}: \mathcal{S} \to \mathbb{R}\)</span>，输出是函数${}:  </p>
<p>G()(s)={<em>{s}^{a}+</em>{s^{} } _{s, s<sup>{}}</sup>{a} (s^{})}$</p>
<p>这意味着<span class="math inline">\(G(\mathbf{v})\)</span>其实就是一个策略<span class="math inline">\(\pi\)</span>。</p>
<p>接下来，值得注意的是，对任何值函数<span class="math inline">\(\mathbf{v}\)</span>，满足</p>
<p>_{G()}<span class="math inline">\(\mathbf{v}=\mathbf{B}_{*} \mathbf{v}\)</span></p>
<p>注意<span class="math inline">\(\mathbf{B}_{*} \mathbf{v}\)</span>并不一定满足<span class="math inline">\(\mathbf{B}_{*} \mathbf{v} = \mathbf{v}\)</span>，上面的式子其实就是做了<span class="math inline">\(\underset{a}{\arg \max }\$和\)</span>$的转换。</p>
<p>Contraction<span class="math inline">\(and Monotonicity of\)</span>Operators</p>
<p>接下来我们定义<span class="math inline">\(\gamma -\text{contraction operators}\)</span>，如果一个算子<span class="math inline">\(\mathbf{B}\)</span>满足，对任意两个值函数<span class="math inline">\(\mathbf v_1,\mathbf v_2\)</span>，有</p>
<p>|<span class="math inline">\(\mathbf{v}_{1}-\mathbf{B}\mathbf{v}_{2}\right\|_{\infty} \leq \gamma\left\|\mathbf{v}_{1}-\mathbf{v}_{2}\right\|_{\infty}\)</span></p>
<p>那么我们说这个算子<span class="math inline">\(\mathbf{B}\)</span>在<span class="math inline">\(L^{\infty}\)</span>上是<span class="math inline">\(\gamma -\text{contraction operators}\)</span>。</p>
<p>可以证明，前面的$<em>{} <span class="math inline">\(和\)</span></em>{*}$都满足上面的性质。</p>
<p>我们可以对离散的情况做一个简单的证明，因为状态空间<span class="math inline">\(\mathcal S\)</span>是一个有限集，任何策略的值函数都可以表示为<span class="math inline">\(\mathbf v_{\pi} = \{\mathbf v_{\pi}(s_1),\mathbf v_{\pi}(s_2),\cdots,\mathbf v_{\pi}(s_n)\}\)</span></p>
<p>而策略状态转移矩阵$ _{} <span class="math inline">\(则是一个\)</span>nn$的矩阵。</p>
<p>对于<span class="math inline">\(L^{\infty}\)</span>，一个有限向量<span class="math inline">\(\mathbf x\)</span>的<span class="math inline">\(L^{\infty}\)</span>其实就是<span class="math inline">\(\mathbf x\)</span>中最大的元素，也就是说</p>
<p>||_{}<span class="math inline">\(= \max_i \mathbf x_i\)</span></p>
<p>实际上，把<span class="math inline">\(\mathbf x\)</span>看作一个集合，<span class="math inline">\(L^{\infty}\)</span>就是在衡量这个集合的直径。</p>
<p>接下来，我们根据上面的说明做一个简单的证明。</p>
<p>对于Bellman$Policy Operator _{} $，有</p>
<p>\begin{aligned}$ |<em>{} </em>{1}-<em>{} </em>{2}|<em>{}&amp;=| </em>{}+<em>{}  - </em>{}+<em>{} |</em>{} \ &amp;=| <em>{}  - </em>{} |<em>{} \ &amp;=| </em>{} ( - )|_{} \ &amp;=<em>i</em>{} ( - )<em>i\ &amp; <em>i ( - )<em>i \ &amp; =|</em>{1}-</em>{2}|</em>{}\ \end{aligned}$</p>
<p>中间那个小于等于号很显然，是因为<span class="math inline">\(\mathbf{P}_{\pi}\)</span>是一个状态转移矩阵，每行的值都小于<span class="math inline">\(1\)</span>，而合等于$1 <span class="math inline">\(，也就是说乘上\)</span>_{}$相当于做了一次加权平均，自然不会让变得更大。</p>
<p>对于Bellman<span class="math inline">\(Optimality Operator \mathbf{B}_{*}\)</span>，我们先说明<span class="math inline">\(\mathbf{B}_{*} \mathbf{v}\)</span>的形式，在<span class="math inline">\(\mathbf{v}\)</span>是有限集的情况下，<span class="math inline">\(\mathbf{B}_{*} \mathbf{v}\)</span>可以被表示成一个向量</p>
<p>_{*}<span class="math inline">\(\mathbf{v} = \left(\max _{a}\left\{\mathcal{R}_{s _1}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s_1, s^{\prime}}^{a} \cdot \mathbf{v}\left(s^{\prime}\right)\right\}, \\ \max _{a}\left\{\mathcal{R}_{s_2}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s_2, s^{\prime}}^{a} \cdot \mathbf{v}\left(s^{\prime}\right)\right\},\\ \cdots\\ \max _{a}\left\{\mathcal{R}_{s_n}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s_n, s^{\prime}}^{a} \cdot \mathbf{v}\left(s^{\prime}\right)\right\}\right)\)</span></p>
<p>那么对于$_{*}  - _{*}  $，有</p>
<p>_{*}<span class="math inline">\(\mathbf{v_1} - \mathbf{B}_{*} \mathbf{v_2} =\\ \left(\max _{a}\left\{\mathcal{R}_{s _1}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s_1, s^{\prime}}^{a} \cdot \mathbf{v_1}\left(s^{\prime}\right)\right\} - \max _{a}\left\{\mathcal{R}_{s _1}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s_1, s^{\prime}}^{a} \cdot \mathbf{v_2}\left(s^{\prime}\right)\right\}, \\ \max _{a}\left\{\mathcal{R}_{s_2}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s_2, s^{\prime}}^{a} \cdot \mathbf{v_1}\left(s^{\prime}\right)\right\}-\max _{a}\left\{\mathcal{R}_{s_2}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s_2, s^{\prime}}^{a} \cdot \mathbf{v_2}\left(s^{\prime}\right)\right\},\\ \cdots\\ \max _{a}\left\{\mathcal{R}_{s_n}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s_n, s^{\prime}}^{a} \cdot \mathbf{v_1}\left(s^{\prime}\right)\right\}-\max _{a}\left\{\mathcal{R}_{s_n}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s_n, s^{\prime}}^{a} \cdot \mathbf{v_2}\left(s^{\prime}\right)\right\}\right)\)</span></p>
<p>注意，实际上<span class="math inline">\(\max_a\)</span>可以看作对一个长度为<span class="math inline">\(|\mathcal A |\)</span>的向量用<span class="math inline">\(L^{\infty}\)</span>，也就是<span class="math inline">\(\max_a\)</span>满足三角不等式，于是，对于$_{*}  - _{*}  $的每一个元素，有</p>
<p>\begin{aligned}$ &amp;<em>{a}{</em>{s_n}^{a}+<em>{s^{} } </em>{s_n, s<sup>{}}</sup>{a} (s<sup>{})}-<em>{a}{</em>{s_n}</sup>{a}+<em>{s^{} } </em>{s_n, s<sup>{}}</sup>{a} (s^{})} \ &amp;<em>{a}{</em>{s_n}^{a}+<em>{s^{} } </em>{s_n, s<sup>{}}</sup>{a} (s<sup>{})-<em>{s_n}^{a}+</em>{s</sup>{} } <em>{s_n, s<sup>{}}</sup>{a} (s^{})}\ &amp;=</em>{a}{<em>{s^{} } </em>{s_n, s<sup>{}}</sup>{a} ( (s<sup>{})-(s</sup>{}) )}\ \end{aligned}$</p>
<p>同时，无论怎样选择<span class="math inline">\(a\)</span>，<span class="math inline">\(\mathcal{P}_{s, s^{\prime}}^{a}\)</span>都是一个概率转移矩阵，和<span class="math inline">\(\mathbf{P}_{\pi}\)</span>有类似的特点，相当于取加权，于是同理得到</p>
<p>|<em>{<em>}$ <em>{1}-</em>{</em>} </em>{2}|<em>{} |</em>{1}-<em>{2}|</em>{}$</p>
<p>证明了$<em>{} <span class="math inline">\(和\)</span></em>{<em>}<span class="math inline">\(都是\)</span>-<span class="math inline">\(，由Contraction\)</span>Mapping<span class="math inline">\(Theorem，我们可以得到\)</span><em>{} <span class="math inline">\(和\)</span></em>{</em>}$都含有唯一的不动点。</p>
<p>接下来，我们定义值函数之间的小于，如果<span class="math inline">\(\mathbf{v}_{1}\leq \mathbf{v}_{2}\)</span>，意味着对于所有的<span class="math inline">\(s\)</span>满足</p>
<p>_{1}(s)<span class="math inline">\(\leq \mathbf{v}_{2}(s)\)</span></p>
<p>不难证明</p>
<p>\begin{array}{l}$ <em>{1} </em>{2} <em>{} </em>{1} <em>{} </em>{2} \ <em>{1} </em>{2} <em>{<em>} <em>{1} </em>{</em>} </em>{2} \end{array}$</p>
<p>也就是说$<em>{} <span class="math inline">\(和\)</span></em>{*}$都是单调的。</p>
<p>Policy Evaluation</p>
<p>有了上面的推导，我们就可以很直观的看待策略评估的过程了。</p>
<p>对于任何策略<span class="math inline">\(\pi\)</span>，我们可以得到$<em>{} <span class="math inline">\(，那么策略评估就是在找到\)</span></em>{} $的不动点。</p>
<p>根据Contraction<span class="math inline">\(Mapping\)</span>Theorem，对任何值函数<span class="math inline">\(\mathbf v\)</span>都有</p>
<p><span class="math inline">\(_{N \to \infty} \mathbf{B}_{\pi}^{N} \mathbf{v}=\mathbf{v}_{\pi}\)</span></p>
<p>这保证了我们可以从一个随机的起点出发，不断按照Bellman方程更新值函数<span class="math inline">\(\mathbf v\)</span>，从而收敛到<span class="math inline">\(\mathbf{v}_{\pi}\)</span>。</p>
<p>Policy Improvement</p>
<p>接下来我们分析策略提升的过程。</p>
<p>在第<span class="math inline">\(k\)</span>次迭代中，我们计算得到了当前的策略<span class="math inline">\({\pi_{k}}\)</span>，对这个策略进行策略评估，我们得到相应的值函数 <em>{</em>{k}}。</p>
<p>那么策略提升的过程就是令<span class="math inline">\(\pi_{k+1}\)</span>满足</p>
<p><em>{k+1}=G(</em>{_{k}})</p>
<p>现在我们想证明存在<span class="math inline">\(\pi_{k+1} \geq \pi_{k}\)</span>，从而保证策略提升一定会让策略变得更好。</p>
<p>还记得我们之前说</p>
<p>_{G()}<span class="math inline">\(\mathbf{v}=\mathbf{B}_{*} \mathbf{v}\)</span></p>
<p>于是有</p>
<p>_{*}<span class="math inline">\(\mathbf{v}_{\pi_{\mathbf{k}}}=\mathbf{B}_{G\left(\mathbf{v}_{\pi_{\mathbf{k}}}\right)} \mathbf{v}_{\pi_{\mathbf{k}}}=\mathbf{B}_{\pi_{k+1}} \mathbf{v}_{\pi_{\mathbf{k}}}\)</span></p>
<p>根据定义，显然对所有对值函数<span class="math inline">\(\mathbf v\)</span>有<span class="math inline">\(\mathbf{B}_{*} \mathbf{v} \geq \mathbf{B}_{\pi} \mathbf{v}\)</span>，于是</p>
<p>_{*}<span class="math inline">\(\mathbf{v}_{\pi_{k}} \geq \mathbf{B}_{\pi_{k}} \mathbf{v}_{\pi_{k}}=\mathbf{v}_{\pi_{k}}\)</span></p>
<p>结合上面的式子得到</p>
<p><em>{</em>{k+1}}<span class="math inline">\(\mathbf{v}_{\pi_{\mathbf{k}}} \geq \mathbf{v}_{\pi_{\mathbf{k}}}\)</span></p>
<p>这说明对值函数进行$<em>{</em>{k+1}} $算子的运算是一个单调递增的过程，也就是说</p>
<p><em>{</em>{k+1}}^{N}<span class="math inline">\(\mathbf{v}_{\pi_{\mathbf{k}}} \geq \ldots \mathbf{B}_{\pi_{k+1}}^{2} \mathbf{v}_{\pi_{\mathbf{k}}} \geq \mathbf{B}_{\pi_{k+1}} \mathbf{v}_{\pi_{\mathbf{k}}} \geq \mathbf{v}_{\pi_{\mathbf{k}}}\)</span></p>
<p>而又有</p>
<p><em>{</em>{+1}}=<span class="math inline">\(_{N \to \infty} \mathbf{B}_{*}^{N} \mathbf{v}_{\pi_{\mathrm{k}}}=\lim _{N \to \infty} \mathbf{B}_{\pi_{k+1}}^{N} \mathbf{v}_{\pi_{\mathrm{k}}}\)</span></p>
<p>于是</p>
<p><em>{</em>{+1}}=$<em>{N } </em>{<em>{k+1}}^{N} </em>{<em>{}} </em>{_{}}</p>
<p>Policy$Iteration</p>
<p>我们之前完成了策略提升的证明，在第<span class="math inline">\(k + 1\)</span>个策略迭代中，我们可以保证单调，也就是<span class="math inline">\(\mathbf{v}_{\pi_{\mathbf{k}+1}} \geq \mathbf{v}_{\pi_{\mathbf{k}}}\)</span>。</p>
<p>于是，当<span class="math inline">\(\mathbf{v}_{\pi_{\mathbf{k}+1}}=\mathbf{v}_{\pi_{\mathbf{k}}}\)</span>，我们就找到了<span class="math inline">\(\mathbf{v}_{*}\)</span>，这是因为$_{<em>} <em>{} <span class="math inline">\(，同时\)</span></em>{</em>}$又只有一个不动点。</p>
<p>Value$Iteration</p>
<p>Value<span class="math inline">\(Iteration的思想就是干脆绕开\)</span>B_<span class="math inline">\(，直接让\)</span>B_*$参与运算。</p>
<p>因为我们知道$_{*} <span class="math inline">\(有唯一的不动点\)</span>_{*}<span class="math inline">\(满足\)</span>_{*} _{*}=_{*}<span class="math inline">\(，而\)</span>_{*} <span class="math inline">\(又是一个单调递增的运算，因为对于任意的值函数\)</span>v$，有</p>
<p><span class="math inline">\(_{N \to \infty} \mathbf{B}_{*}^{N} \mathbf{v}=\mathbf{v}_{*}\)</span></p>
<p>这保证了值迭代的可行性。</p>
<p>Greedy<span class="math inline">\(Policy from Optimal VF is an Optimal\)</span>Policy</p>
<p>最后要证明的是Greedy<span class="math inline">\(Policy from Optimal VF is an Optimal\)</span>Policy。</p>
<p>其实很简单了，我们已经得到了最优值函数<span class="math inline">\(\mathbf v_*\)</span>，我们贪心的选择策略，也就是</p>
<p>_*<span class="math inline">\(= G(\mathbf v_*)\)</span></p>
<p>那么根据最优值函数的定义，我们只需要反过来证明<span class="math inline">\(\mathbf v_{\pi_*}= \mathbf v_*\)</span>即可。</p>
<p>首先我们有</p>
<p><em>{G(</em>{*})}<span class="math inline">\(\mathbf{v}_{*}=\mathbf{B}_{*} \mathbf{v}_{*}\)</span></p>
<p>因<span class="math inline">\(\mathbf v_*\)</span>是不动点，因此满足</p>
<p><em>{G(</em>{*})}<span class="math inline">\(\mathbf{v}_{*}=\mathbf{B}_{*} \mathbf{v}_{*} = \mathbf{v}_{*}\)</span></p>
<p>这意味着<span class="math inline">\(\mathbf{v}_{*}\)</span>也是<span class="math inline">\(\mathbf{B}_{G\left(\mathbf{v}_{*}\right)}\)</span>的不动点。而<span class="math inline">\(\mathbf{B}_{G\left(\mathbf{v}_{*}\right)}\)</span>又只有一个不动点<span class="math inline">\(\mathbf{v}_{G\left(\mathbf{v}_{*}\right)}\)</span>，因此</p>
<p>_{<em>}=<em>{G(</em>{</em>})}</p>
<p>即</p>
<p><span class="math inline">\(v_{\pi_*}= \mathbf v_*\)</span></p>
]]></content>
  </entry>
  <entry>
    <title>这个网站被开通了</title>
    <url>/2020/12/08/%E8%BF%99%E4%B8%AA%E7%BD%91%E7%AB%99%E8%A2%AB%E5%BC%80%E9%80%9A%E4%BA%86/</url>
    <content><![CDATA[<p>用了一堆奇奇怪怪的技术让我闲置了小半年的服务器和域名发挥余热。准备这段时间把知乎和其他地方的文章陆陆续续整理过来，算是做个汇总。</p>
<a id="more"></a>
<p>网站现在被托管在阿里云的一个小服务器上，人生第一次见到一核的CPU，还以为htop出bug了。考虑有机会把它搞到自己的机器上，不过要等我分到一个固定ip再说。说起来，最近看到一个想法， 说可以把域名直接映射到ipv6上，然后分配ipv6，这样就省去DNS服务了，amazing。</p>
<p>同时，这个网站还被我同步到了github page和gitee page，具体的教程应该在下一篇文章，两个网站的网址是franktiantt.github.io和franktian424.gitee.io。</p>
<p>就这样吧。</p>
]]></content>
      <categories>
        <category>唠嗑</category>
      </categories>
      <tags>
        <tag>开张撒花</tag>
        <tag>白嫖</tag>
      </tags>
  </entry>
</search>
